# Optimize ML Models and Deploy Human-in-the-Loop Pipelines

This repository contains materials and code related to the course "Optimize ML Models and Deploy Human-in-the-Loop Pipelines." In this course, I explored various aspects of optimizing machine learning models and deploying them with human-in-the-loop pipelines.

## Lab 1: Optimize models using Automatic Model Tuning

### Learning Objectives
- Describe practical challenges related to data for machine learning, including scale and resource constraints (storage, compute, memory).
- Explain approaches to address challenges with data for machine learning.
- Describe practical challenges related to ML training, such as large datasets and optimizing training time and costs.
- Explain approaches to address ML training challenges using techniques like streaming data, profiling training infrastructure monitoring, data parallelism, etc.
- Describe the concept of hyper-parameter tuning.
- Explain popular algorithms used for automatic model tuning.
- Apply hyper-parameter tuning to a BERT-based text classifier and dataset.
- Describe the concept of model evaluation.
- Demonstrate how to evaluate a model.

In this lab, I learned how to overcome data-related challenges in machine learning, optimize model training, and fine-tune models using automatic hyper-parameter tuning. I also gained insights into model evaluation techniques.

## Lab 2: Deploy models with A/B testing and monitor model performance

### Learning Objectives
- Describe practical challenges related to ML deployment and monitoring.
- Explain approaches to address deployment and monitoring challenges, including autoscaling, automating model quality monitoring, and automating retraining pipelines.
- Evaluate model deployment options and considerations.
- Describe common deployment challenges and strategies.
- Explain considerations for monitoring a machine learning workload.
- Describe approaches to model monitoring.
- Discuss how to use Amazon SageMaker to perform A/B testing.
- Demonstrate deploying a model version to Amazon SageMaker using A/B testing.

In this lab, I explored the challenges associated with deploying machine learning models and monitoring their performance. I learned techniques for A/B testing, automated monitoring, and deployment strategies.

## Lab 3: Label data at scale using private human workforces and build human-in-the-loop pipelines

### Learning Objectives
- Describe the importance of data labeling.
- Discuss the concept of automatic data labeling and active learning.
- Implement human-workforce data labeling tasks with SageMaker Ground Truth.
- Discuss the concept of human-in-the-loop pipelines.
- Implement human-in-the-loop pipelines with Amazon AugmentedAI (Amazon A2I).

In this lab, I delved into the critical task of data labeling, understanding the importance and techniques for efficient data annotation. I also explored the concept of human-in-the-loop pipelines, using SageMaker Ground Truth and Amazon A2I to incorporate human feedback in the machine learning process.



